# Usefull Questions

1. TOC
{:toc}

## Modeling and Feature Selection


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
df = pd.read_csv('appliances_cleaned_final.csv')
df = df[df['a_re_joined'].isnull() == False]
```


```python
from sklearn import metrics
from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score
from sklearn.metrics import recall_score, f1_score, balanced_accuracy_score

def produce_confusion(positive_label, negative_label, cut_off, df, 
                      y_pred_name, y_real_name):

    #Set pred to 0 or 1 depending on whether it's higher than the cut_off point.
    df['pred_binary'] = np.where(df[y_pred_name] > cut_off , 1, 0)
    
    #Metrics
    acc = accuracy_score(y_real_name, df['pred_binary'])
    bacc = balanced_accuracy_score(y_real_name, df['pred_binary'])
    prec = precision_score(y_real_name, df['pred_binary'])
    rec = recall_score(y_real_name, df['pred_binary'])
    f1 = f1_score(y_real_name, df['pred_binary'])

    #Build the CM
    cm = confusion_matrix(y_real_name, df['pred_binary'])

    ax= plt.subplot()
    sns.heatmap(cm, annot=True, ax=ax, fmt='g')

    # labels, title, ticks
    ax.set_xlabel('Predicted labels')
    ax.set_ylabel('Real labels')
    ax.set_title('Confusion Matrix')
    ax.xaxis.set_ticklabels([negative_label, positive_label])
    ax.yaxis.set_ticklabels([negative_label, positive_label])
    plt.show()

    print(f'The test accuracy is {acc}, the test precision is {prec},the test recall is {rec}, the test f1 score is {f1}, the balanced accuracy is {bacc}')
    
    #ROC Curves
    fpr, tpr, thresholds = roc_curve(y_real_name, df['target_pred'])
    roc_auc = auc(fpr, tpr)
    display = RocCurveDisplay(fpr = fpr, tpr = tpr, roc_auc = roc_auc, estimator_name='Target Predictions')

    display.plot()
    plt.plot([0,1], [0,1], 'k--')
    plt.show()
    pass
```

### Feature Selection

First, we removed all the columns containing text as that can't be handled by the model. Note: a_re_joined is left in for now so that the count vectoriser can derive new features from it, but it is dropped before the model is fit.

Then we removed the columns q_weight and a_weight as they only contained zeros.

Finally, using the summary from the stats-model Logistic Regression model, we removed features that had a p-value less than 0.1.


```python
from sklearn.model_selection import train_test_split
import statsmodels.api as sm

cols = ['answer_length', 'q_upper', 'a_upper', 'q_model',
       'q_do', 'a_model', 'a_imp_dim', 'q_pos_sentiment', 
       'a_pos_sentiment', 'q_adverbs', 'q_determiners',
       'q_nouns', 'q_numerals', 'a_adpositions', 
       'a_pronouns', 'a_re_joined', 'q_punc_count', 
       'a_punc_ratio', 'jaccard', 'lev_distance']

X = df[cols]
X = sm.add_constant(X)
y = df['our_label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 142, stratify = y)

X_train.reset_index(drop = False, inplace = True)
X_test.reset_index(drop = False, inplace = True)

``` 

We use the CountVectorizer with 300 features which we found to give the best recall after trialling varying numbers.

We fit the CountVectorizer, transform X_test then merge our resulting features back onto the main dataframes.



```python
from sklearn.feature_extraction.text import CountVectorizer


vectorizer = CountVectorizer(analyzer = "word",
                             tokenizer = None,
                             preprocessor = None,
                             stop_words = None,
                             max_features = 300)

train_features = vectorizer.fit_transform(X_train['a_re_joined'])
train_features = train_features.toarray()
df1 = pd.DataFrame(train_features)

test_features = vectorizer.transform(X_test['a_re_joined'])
test_features = test_features.toarray()
df2 = pd.DataFrame(test_features)

X_train = X_train.merge(df1, how = 'inner', left_index = True, right_index = True)            
X_test = X_test.merge(df2, how = 'inner', left_index = True, right_index = True)

X_train = X_train.drop(columns = 'a_re_joined').set_index('index')                
X_test = X_test.drop(columns = 'a_re_joined').set_index('index')                

```

Now that our DataFrame is prepared we can use statsmodels to find our important features.


```python
import statsmodels.tools

results = sm.Logit(y_train, X_train).fit()
summary = results.summary()

X_train['target_pred'] = results.predict(X_train)

X_test['target_pred'] = results.predict(X_test)

print(f'For the train dataset:')
print(produce_confusion('Useless', 'Useful', 0.5, X_train, 'target_pred', y_train))

print(f'For the test dataset:')
print(produce_confusion('Useless', 'Useful', 0.5, X_test, 'target_pred', y_test))

print(summary)
```

    Optimization terminated successfully.
             Current function value: 0.298087
             Iterations 8
    For the train dataset:
    


   
![png](/images/output_8_1.png)
    


    The test accuracy is 0.8601583113456465, the test precision is 0.5408426483233018,the test recall is 0.5707803992740472, the test f1 score is 0.5554083885209713, the balanced accuracy is 0.7416125311667826
    


    
![png](/images/output_8_3.png)
    


    None
    For the test dataset:
    


    
![png](/images/output_8_5.png)
    


    The test accuracy is 0.827318156579678, the test precision is 0.445141065830721,the test recall is 0.5144927536231884, the test f1 score is 0.47731092436974787, the balanced accuracy is 0.6992135899263483
    


    
![png](/images/output_8_7.png)
    


    None
                               Logit Regression Results                           
    ==============================================================================
    Dep. Variable:              our_label   No. Observations:                 7201
    Model:                          Logit   Df Residuals:                     6881
    Method:                           MLE   Df Model:                          319
    Date:                Tue, 15 Mar 2022   Pseudo R-squ.:                  0.3034
    Time:                        10:19:15   Log-Likelihood:                -2146.5
    converged:                       True   LL-Null:                       -3081.6
    Covariance Type:            nonrobust   LLR p-value:                3.192e-217
    ===================================================================================
                          coef    std err          z      P>|z|      [0.025      0.975]
    -----------------------------------------------------------------------------------
    const              -2.8649      0.339     -8.454      0.000      -3.529      -2.201
    answer_length      -0.0027      0.001     -2.848      0.004      -0.005      -0.001
    q_upper             0.8356      0.457      1.829      0.067      -0.060       1.731
    a_upper            -1.9901      0.584     -3.410      0.001      -3.134      -0.846
    q_model             0.3796      0.118      3.222      0.001       0.149       0.611
    q_do               -0.1721      0.095     -1.803      0.071      -0.359       0.015
    a_model             0.5792      0.263      2.202      0.028       0.064       1.095
    a_imp_dim          -0.8802      0.253     -3.472      0.001      -1.377      -0.383
    q_pos_sentiment     0.7744      0.343      2.259      0.024       0.103       1.446
    a_pos_sentiment     0.7950      0.332      2.392      0.017       0.144       1.446
    q_adverbs           2.0391      0.885      2.304      0.021       0.305       3.774
    q_determiners       1.4054      0.643      2.187      0.029       0.146       2.665
    q_nouns             2.0591      0.404      5.096      0.000       1.267       2.851
    q_numerals          1.8164      0.771      2.355      0.019       0.305       3.328
    a_adpositions       1.9139      0.709      2.698      0.007       0.523       3.304
    a_pronouns          2.1743      0.800      2.719      0.007       0.607       3.742
    q_punc_count        0.0818      0.030      2.685      0.007       0.022       0.141
    a_punc_ratio       -3.6899      1.870     -1.973      0.048      -7.355      -0.024
    jaccard            -2.8094      0.673     -4.171      0.000      -4.129      -1.489
    lev_distance        0.1447      0.061      2.358      0.018       0.024       0.265
    0                  -0.1326      0.374     -0.354      0.723      -0.866       0.601
    1                  -0.4470      0.505     -0.886      0.376      -1.436       0.542
    2                   0.4347      0.370      1.174      0.240      -0.291       1.160
    .
    .
    .
    ===================================================================================
    

We wont go through the whole summary as it is a bit long! We uses the P>(z) to determine statistically significant features to retain.

### Weighted Model


```python
X_train.drop(columns = ['const', 'target_pred', 'pred_binary'], inplace = True)
X_test.drop(columns = ['const', 'target_pred', 'pred_binary'], inplace = True)
```

After using the stats-models Logistic Regression model to select features, we used the Sklearn Logistic Regression model to allow us to weight the data as we have imbalanced classes.

We scaled the data to speed up convergence of the model.


```python
from sklearn.preprocessing import MaxAbsScaler

scaler = MaxAbsScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.transform(X_test))
```

For our weighting, we counted the proportion of observations that belonged to each class and turned this into a percentage.



```python
from sklearn.linear_model import LogisticRegression

w1, w2 = 100*df['our_label'].value_counts()/df.shape[0]
w = {0:w2, 1:w1}

results = LogisticRegression(class_weight=w, max_iter = 400).fit(X_train, y_train)

X_train['target_pred'] = results.predict_proba(X_train)[:,1]

X_test['target_pred'] = results.predict_proba(X_test)[:,1]

print(f'For the train dataset:')
print(produce_confusion('Useless', 'Useful', 0.5,  X_train, 'target_pred', y_train))

print(f'For the test dataset:')
print(produce_confusion('Useless', 'Useful', 0.5, X_test, 'target_pred', y_test))
```

    For the train dataset:
    


    
![png](/images/output_15_1.png)
    


    The test accuracy is 0.779613942507985, the test precision is 0.39293598233995586,the test recall is 0.8076225045372051, the test f1 score is 0.5286605286605287, the balanced accuracy is 0.7910878549903602
    


    
![png](/images/output_15_3.png)
    


    
    For the test dataset:
    


    
![png](/images/output_15_5.png)
    


    The test accuracy is 0.7284841754580789, the test precision is 0.31918505942275044,the test recall is 0.6811594202898551, the test f1 score is 0.4346820809248555, the balanced accuracy is 0.709104300308862
    


    
![png](/images/output_15_7.png)
    


    
    

## Pipeline implimentation


### Loading the data


```python
import pandas as pd
import numpy as np
```


```python
data = pd.read_csv('appliances_cleaned_final.csv')
data.dropna(inplace=True)
```

### Determine feature types


```python
def feature_catogoriser(df, target):
    num_features = []
    text_features = []
    for feature in df.columns:
        if ((df[feature].dtype) == 'O') & (feature != target):
            text_features.append(feature)
        elif feature != target:
            num_features.append(feature)

    return text_features, num_features
```

The features catogoriser runs over all features in the data frame, labeling Object types as
text columns and others as numerical columns.

We then give a list of features to be used (determined by our model development) and use the 
feature catogoriser to sort them.


```python
cols = ['answer_length', 'q_upper', 'a_upper', 'q_model',
       'q_do', 'a_model', 'a_imp_dim', 'q_pos_sentiment', 'a_pos_sentiment',
       'q_adverbs', 'q_determiners',
       'q_nouns', 'q_numerals', 'a_adpositions', 
       'a_pronouns', 'a_re_joined', 'q_punc_count', 'a_punc_ratio',
       'jaccard', 'lev_distance', 'our_label']


text_features, num_features = feature_catogoriser(data[cols], 'our_label')
```

### Fitting and Evaluating Model

The next series of functions run an experiment and output the model and an evaluation of it.

Experimenter takes our data and feature information along with a dictionary of classifiers 
and some other parameters. It performs our train/test split then runs and evaluates the 
model using each classifier in turn. We experimented with taking an ensemble aproach 
at this stage but stripped it out for readability as it was not used in our final model.


```python
from sklearn.model_selection import train_test_split
from sklearn import metrics


def experimenter(df, target, classifiers,threshold=0.5, text_features=[], num_features=[], 
                 stop_words=None, n_gram=(1,2), chi_k=300):
    features = text_features + num_features
    X = data[features]
    y = data[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=142, stratify=y)
    y_test = y_test.to_frame()
    
    out = pd.DataFrame()
    
    for key, value in classifiers.items():  #for each classifier provided run model and produce scores
        y_test[value], pl = modeling_f(X_train, X_test, y_train, key, features=features, 
                                  chi_k = chi_k, n_gram=n_gram)
        prediction = np.where(y_test[value] > threshold , 1, 0)
        out.loc[value,'accuracy'] = round(metrics.accuracy_score(y_test[target], prediction),3)
        out.loc[value,'precision'] = round(metrics.precision_score(y_test[target], prediction ),3)
        out.loc[value,'recall'] = round(metrics.recall_score(y_test[target], prediction),3)
        out.loc[value,'auc'] = round(metrics.roc_auc_score(y_test[target], y_test[value]),3)
    
    return out, pl
    
```

### The Pipeline

We quickly define two functions which sort the columns into numerical and textual for textual features
we combine them into a single string (though in our final model we only use a single textual string).


```python
def select_num_columns(df):
    
    out = df.loc[:, num_features]
    return out
```


```python
def combine_text_columns(df):

    out = df.loc[:, text_features]
    out['text'] =""
    for col in df.columns:
        if col in text_features:
            out.text += " "
            out.text += df[col]
    return out.text
```

We then run the model. In cases where there is only numeric data this is simple - we scale the data and run
the model. If howerver there is textual data the process is more involved:
- We use a feature union to combine two sub pipelines
- The first simply selects and returns our numeric data
- The second selects text data, count vectorises it and then uses SelectKBest to take the most useful


```python
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import chi2, SelectKBest
from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler

def modeling_f(X_train, X_test, y_train, classifier, features, n_gram=(1,2),
               name='experiment', chi_k=300):
    
    # Perform preprocessing
    get_text_data = FunctionTransformer(combine_text_columns, validate=False)
    get_numeric_data = FunctionTransformer(select_num_columns, validate=False)
    
    # Instantiate pipeline: pl
    if text_features == []:
        pl = Pipeline([
                ('scale', MaxAbsScaler()),
                ('clf', classifier)
            ])
    else:
        pl = Pipeline([
                ('union', FeatureUnion(
                    transformer_list = [
                        ('numeric_features', Pipeline([
                            ('selector', get_numeric_data),
                        ])),
                        ('text_features', Pipeline([
                            ('selector', get_text_data),
                            ('vectorizer', CountVectorizer(analyzer = 'word',
                                                           max_features=350,
                                                           ngram_range = n_gram)),
                            ('dim_red', SelectKBest(chi2, k =chi_k))
                        ]))
                     ]
                )),
                ('scale', MaxAbsScaler()),
                ('clf', classifier)
            ])
    
    # Train test split
    X_train = X_train[features]
    X_test = X_test[features]
               
    # Make predictions
    pl.fit(X_train, y_train)
    pred = pl.predict_proba(X_test)[:,1]

    return pred, pl
```

We fit the pipeline on the training data and make our predictions



### Running it

First our Classifier:


```python
from sklearn.linear_model import LogisticRegression

w1, w2 = 100*data['our_label'].value_counts()/data.shape[0]
w = {0:w2, 1:w1}

classifiers = {LogisticRegression(class_weight=w, max_iter=400):'lr'}
```

Now we run our experiment:


```python
out, pl = experimenter(df=data, target='our_label',classifiers=classifiers, 
                       text_features=text_features, num_features=num_features, 
                       chi_k=100, n_gram=(1,3))
print(out)
```

        accuracy  precision  recall    auc
    lr     0.743      0.343   0.739  0.824
    

and finally the result is pickled


```python
from joblib import dump

dump(pl, 'amazon_model.joblib') 
```




    ['amazon_model.joblib']



## Class and Function



### The ideal:

If we were able to impliment "helpfullness rating" as an object, we would be able to initialise within the object our pipeline and the methods which act on both question and answer. This in turn would improve the speed of our function particularly when run on full DataFrames.

We would also be able to initialise different objects with different pipeline inputs to make predictions about different categories of data.

### Function 

Our helpfullness function is a workaround which takes:
- a question string
- an answer string
- a probability threshold which allows us to finetune the models output.
- simple_test: a Boolian which we used for debuging single q/a pairs.

The function creates question and answer objects in order to be able to access thier atributes. It also calculates the jaccard and lev distance. 

We build a DataFrame from a dictionary and load our pickled pipeline which is then used to make a prediction.


```python
def helpfulness_rating(question, answer, threshold=0.5, simple_test=False):
    Q = Helper(question)
    A = Helper(answer)
    lev_matrix = [lev(i, j) for i in Q.tokens for j in A.tokens]
    list_size = [len(Q.tokens), len(A.tokens)]
    final = []
    for i in range(min(list_size)):
        final.append(min(lev_matrix))
        lev_matrix.remove(min(lev_matrix))
    lev_distance = sum(final) / len(final)
    common_words =  Q.unique_words.intersection(A.unique_words)
    jaccard = len(common_words) / (len(Q.unique_words) + len(A.unique_words)) - len(common_words)
    
    # 
    
    model_input = {
        'answer_length': [A.length],
        'q_upper': [Q.upper_ratio],
        'a_upper': [A.upper_ratio] ,
        'q_model': [Q.mention_model],
        'q_do': [Q.asks_do],
        'a_model': [A.mention_model],
        'a_imp_dim': [A.imperial_dimensions],
        'q_pos_sentiment': [Q.positive_sentiment],
        'a_pos_sentiment': [A.positive_sentiment],
        'q_adverbs': [Q.adverb_ratio],
        'q_determiners': [Q.determiner_ratio],
        'q_nouns': [Q.noun_ratio],
        'q_numerals': [Q.numeral_ratio],
        'a_adpositions': [A.adposition_ratio],
        'a_pronouns': [A.pronoun_ratio],
        'a_re_joined': [A.cleaned],
        'q_punc_count': [len(Q.punctuation_marks)],
        'a_punc_ratio': [A.punctuation_mark_ratio],
        'jaccard': [jaccard],
        'lev_distance': [lev_distance],
    }
    X_test = pd.DataFrame.from_dict(model_input)

    text_features, num_features = feature_catogoriser(X_test, "missingno")
    pl = load('amazon_model.joblib')
    pred = pl.predict_proba(X_test)[:,1]
    result = np.where(pred > threshold , "useless", "useful")

    if simple_test:
        print(f"question: {Q.string}")
        print(f"answer: {A.string}")
        print(f"rating: {result}")
    else:
        return pred
```

### A Class Act

The Helper class was intended as a parent class for our question and answer classes. As things stand the question and answer classes have all atributes whereas in our data this is not the case.

Each Helper object contains all of our regular expressions, stopword lists and feature analysis objects. When we initialise an object it is automatically cleaned and has it's features generated. 

We have also used properties to return features that use regex find. There are two additional methods: 
- The cleaning method
- A number replacement method (not currently implimented)



```python
class Helper:
    question_regex = re.compile(r"(^|\s|\W)[\s\w]+\?(\s|\W|$)")
    url_regex = re.compile(r"(http://|https://)?([a-z0-9][a-z0-9\-]*\.)+(com|co\.uk|net|info|edu|ac.uk|org)(\s|$)")
    model_regex = re.compile(r"(^|\s|\W)[Mm]odel(\s|\W|$)")
    how_regex = re.compile(r"(^|\s|\W)[Hh]ow(\s|\W|$)")
    do_regex = re.compile(r"(^|\s|\W)[Dd]o(es)?(\s|\W|$)")
    where_regex = re.compile(r"(^|\s|\W)[Ww]here(\s|\W|$)")
    dimensions_regex = re.compile(r"(^|\s|\W)([Dd]imensions?|[Mm]easurements?|[Ll](e|o)ng(th)?|[Hh]e?ight?)(\s|\W|$)")
    imp_dim_regex = re.compile(r"(^|\s|\W)\d\w?([\"'`‘’“”]|[Ii]nch(es)?|[Ff](oo|ee)?t)(\s|\W|$)")
    met_dim_regex = re.compile(r"(^|\s|\W)\d\w?(([Cc]enti|[Mm]ili)?[Mm]eters?|[CcMm]?[Mm])(\s|\W|$)")
    can_regex = re.compile(r"(^|\s|\W)[Cc]an(\s|\W|$)")
    fix_regex = re.compile(r"(^|\s|\W)[Ff]ix(ing|ed|es)?(\s|\W|$)")
    work_regex = re.compile(r"(^|\s|\W)[Ww]ork(ing|ed|s)?(\s|\W|$)")
    weight_regex = re.compile(r"(^|\s|\W)[Ww]eihg(t|s|ts)?(\s|\W|$)")
    imp_weight_regex = re.compile(r"([Pp]ounds?|lbS?|[Oo]unces?|ozs?)")
    met_weight_regex = re.compile(r"(([Mm]ili)?[Gg]rams?|[Mm]?[Gg])")
    number_regex = re.compile(r"((?=\b)(one\b|two\b|three\b|four\b|five\b|six\b|seven\b|eight\b|nine\b|ten\b|eleven\b|twelve\b|thirteen\b|fourteen\b|fifteen\b|sixteen\b|seventeen\b|eighteen\b|nineteen\b|twenty\b|thirty\b|fou?rty\b|fifty\b|sixty\b|seventy\b|eighty\b|nine?ty\b|hundred\b|thousand\b|half\b|quarters?\b|eighths?\b)\W?(and (a )?)?)+")
    sentiment = SentimentIntensityAnalyzer()
    p_stemmer = PorterStemmer()
    punc_list = [p for p in string.punctuation]
    stopword = [x for x in stopwords.words('english') if x not in ['no', 'should', 'both', 'not']]

    def __init__(self, string):
        self.string = string
        self.cleaned = self.string
        self.tokens = []
        self.number_replacer()
        self.cleaning()

        self.length = len(self.string)
        self.unique_words = set(self.tokens)
        self.polarity_dict = self.sentiment.polarity_scores(self.string)
        self.negative_sentiment = self.polarity_dict['neg']
        self.positive_sentiment = self.polarity_dict['pos']
        self.neutral_sentiment = self.polarity_dict['neu']
        self.compound_sentiment = self.polarity_dict['compound']
        self.tagword_tuples = pos_tag(self.tokens, tagset='universal')
        self.adjectives = [x[0] for x in self.tagword_tuples if x[1] == 'ADJ']
        self.adpositions = [x[0] for x in self.tagword_tuples if x[1] == 'ADP']
        self.adverbs = [x[0] for x in self.tagword_tuples if x[1] == 'ADV']
        self.conjunctions = [x[0] for x in self.tagword_tuples if x[1] == 'CONJ']
        self.determiners = [x[0] for x in self.tagword_tuples if x[1] == 'DET']
        self.nouns = [x[0] for x in self.tagword_tuples if x[1] == 'NOUN']
        self.numerals = [x[0] for x in self.tagword_tuples if x[1] == 'NUM']
        self.particles = [x[0] for x in self.tagword_tuples if x[1] == 'PRT']
        self.pronouns = [x[0] for x in self.tagword_tuples if x[1] == 'PRON']
        self.verbs = [x[0] for x in self.tagword_tuples if x[1] == 'VERB']
        self.punctuation_marks = [x[0] for x in self.tagword_tuples if x[1] == '.']
        self.unknown_words = [x[0] for x in self.tagword_tuples if x[1] == 'X']
        self.adjective_ratio = len([x[0] for x in self.tagword_tuples if x[1] == 'ADJ']) / len(self.tokens)
        self.adposition_ratio = len([x[0] for x in self.tagword_tuples if x[1] == 'ADP']) / len(self.tokens)
        self.adverb_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'ADV']) / len(self.tokens)
        self.conjunction_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'CONJ']) / len(self.tokens)
        self.determiner_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'DET']) / len(self.tokens)
        self.noun_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'NOUN']) / len(self.tokens)
        self.numeral_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'NUM']) / len(self.tokens)
        self.particle_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'PRT']) / len(self.tokens)
        self.pronoun_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'PRON']) / len(self.tokens)
        self.verb_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'VERB']) / len(self.tokens)
        self.punctuation_mark_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == '.']) / len(self.tokens)
        self.unknown_word_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'X']) / len(self.tokens)


    @property
    def question_mark(self):
        return 1 if self.question_regex.search(self.string) else 0

    @property
    def has_url(self):
        return 1 if self.url_regex.search(self.string) else 0

    @property
    def mention_model(self):
        return 1 if self.model_regex.search(self.string) else 0

    @property
    def asks_how(self):
        return 1 if self.how_regex.search(self.string) else 0

    @property
    def asks_do(self):
        return 1 if self.do_regex.search(self.string) else 0

    @property
    def asks_where(self):
        return 1 if self.where_regex.search(self.string) else 0

    @property
    def mention_dimensions(self):
        return 1 if self.dimensions_regex.search(self.string) else 0

    @property
    def imperial_dimensions(self):
        return 1 if self.imp_dim_regex.search(self.string) else 0

    @property
    def metric_dimensions(self):
        return 1 if self.met_dim_regex.search(self.string) else 0

    @property
    def asks_can(self):
        return 1 if self.can_regex.search(self.string) else 0

    @property
    def mentions_fix(self):
        return 1 if self.fix_regex.search(self.string) else 0

    @property
    def mentions_work(self):
        return 1 if self.work_regex.search(self.string) else 0

    @property
    def mentions_weight(self):
        return 1 if self.weight_regex.search(self.string) else 0

    @property
    def imperial_weight(self):
        return 1 if self.imp_weight_regex.search(self.string) else 0

    @property
    def metric_weight(self):
        return 1 if self.met_weight_regex.search(self.string) else 0

    @property
    def word_count(self):
        return len(self.tokens)

    @property
    def upper_ratio(self):
        count = 0
        for char in self.string:
            try:
                assert char.isascii() and char.isupper()
                count +=1
            except AssertionError:
                pass
        return count / len(self.string)

    def cleaning(self):
        self.cleaned = self.cleaned.lower()
        self.cleaned = contractions.fix(self.cleaned)
        self.cleaned = re.sub(self.url_regex, "", self.cleaned)
        self.tokens = word_tokenize(self.cleaned)
        self.tokens = [self.p_stemmer.stem(x) for x in self.tokens]
        self.tokens = [y for y in self.tokens if y not in self.punc_list and y not in self.stopword]
        if len(self.tokens) == 0:
            self.tokens = ['EMPTY']
        self.cleaned = " ".join(self.tokens)


    def number_replacer(self):
        searching = True
        transformed = self.string
        while searching:
            convert_words = self.number_regex.search(transformed)
            if convert_words:
                replacement = str(w2n.word_to_num(convert_words.group(0))) + " "
                transformed = self.number_regex.sub(replacement, convert_words.string, 1)
            else:
                searching = False
        return transformed
```
