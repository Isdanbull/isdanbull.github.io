# Amazon Project

## Modeling and Feature Selection


```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
df = pd.read_csv('appliances_cleaned_final.csv')
df = df[df['a_re_joined'].isnull() == False]
```


```python
from sklearn import metrics
from sklearn.metrics import roc_auc_score, roc_curve, RocCurveDisplay, auc
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score
from sklearn.metrics import recall_score, f1_score, balanced_accuracy_score

def produce_confusion(positive_label, negative_label, cut_off, df, 
                      y_pred_name, y_real_name):

    #Set pred to 0 or 1 depending on whether it's higher than the cut_off point.
    df['pred_binary'] = np.where(df[y_pred_name] > cut_off , 1, 0)
    
    #Metrics
    acc = accuracy_score(y_real_name, df['pred_binary'])
    bacc = balanced_accuracy_score(y_real_name, df['pred_binary'])
    prec = precision_score(y_real_name, df['pred_binary'])
    rec = recall_score(y_real_name, df['pred_binary'])
    f1 = f1_score(y_real_name, df['pred_binary'])

    #Build the CM
    cm = confusion_matrix(y_real_name, df['pred_binary'])

    ax= plt.subplot()
    sns.heatmap(cm, annot=True, ax=ax, fmt='g')

    # labels, title, ticks
    ax.set_xlabel('Predicted labels')
    ax.set_ylabel('Real labels')
    ax.set_title('Confusion Matrix')
    ax.xaxis.set_ticklabels([negative_label, positive_label])
    ax.yaxis.set_ticklabels([negative_label, positive_label])
    plt.show()

    print(f'The test accuracy is {acc}, the test precision is {prec},the test recall is {rec}, the test f1 score is {f1}, the balanced accuracy is {bacc}')
    
    #ROC Curves
    fpr, tpr, thresholds = roc_curve(y_real_name, df['target_pred'])
    roc_auc = auc(fpr, tpr)
    display = RocCurveDisplay(fpr = fpr, tpr = tpr, roc_auc = roc_auc, estimator_name='Target Predictions')

    display.plot()
    plt.plot([0,1], [0,1], 'k--')
    plt.show()
    pass
```

### Feature Selection

First, we removed all the columns containing text as that can't be handled by the model. Note: a_re_joined is left in for now so that the count vectoriser can derive new features from it, but it is dropped before the model is fit.

Then we removed the columns q_weight and a_weight as they only contained zeros.

Finally, using the summary from the stats-model Logistic Regression model, we removed features that had a p-value less than 0.1.


```python
from sklearn.model_selection import train_test_split
import statsmodels.api as sm

cols = ['answer_length', 'q_upper', 'a_upper', 'q_model',
       'q_do', 'a_model', 'a_imp_dim', 'q_pos_sentiment', 
       'a_pos_sentiment', 'q_adverbs', 'q_determiners',
       'q_nouns', 'q_numerals', 'a_adpositions', 
       'a_pronouns', 'a_re_joined', 'q_punc_count', 
       'a_punc_ratio', 'jaccard', 'lev_distance']

X = df[cols]
X = sm.add_constant(X)
y = df['our_label']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 142, stratify = y)

X_train.reset_index(drop = False, inplace = True)
X_test.reset_index(drop = False, inplace = True)

```

    C:\Users\IsDan\anaconda3\lib\site-packages\statsmodels\tsa\tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only
      x = pd.concat(x[::order], 1)
    

We use the CountVectorizer with 300 features which we found to give the best recall after trialling varying numbers.

We fit the CountVectorizer, transform X_test then merge our resulting features back onto the main dataframes.



```python
from sklearn.feature_extraction.text import CountVectorizer


vectorizer = CountVectorizer(analyzer = "word",
                             tokenizer = None,
                             preprocessor = None,
                             stop_words = None,
                             max_features = 300)

train_features = vectorizer.fit_transform(X_train['a_re_joined'])
train_features = train_features.toarray()
df1 = pd.DataFrame(train_features)

test_features = vectorizer.transform(X_test['a_re_joined'])
test_features = test_features.toarray()
df2 = pd.DataFrame(test_features)

X_train = X_train.merge(df1, how = 'inner', left_index = True, right_index = True)            
X_test = X_test.merge(df2, how = 'inner', left_index = True, right_index = True)

X_train = X_train.drop(columns = 'a_re_joined').set_index('index')                
X_test = X_test.drop(columns = 'a_re_joined').set_index('index')                

```

Now that our DataFrame is prepared we can use statsmodels to find our important features.


```python
import statsmodels.tools

results = sm.Logit(y_train, X_train).fit()
summary = results.summary()

X_train['target_pred'] = results.predict(X_train)

X_test['target_pred'] = results.predict(X_test)

print(f'For the train dataset:')
print(produce_confusion('Useless', 'Useful', 0.5, X_train, 'target_pred', y_train))

print(f'For the test dataset:')
print(produce_confusion('Useless', 'Useful', 0.5, X_test, 'target_pred', y_test))

print(summary)
```

    Optimization terminated successfully.
             Current function value: 0.298087
             Iterations 8
    For the train dataset:
    


    
![png](output_8_1.png)
    


    The test accuracy is 0.8601583113456465, the test precision is 0.5408426483233018,the test recall is 0.5707803992740472, the test f1 score is 0.5554083885209713, the balanced accuracy is 0.7416125311667826
    


    
![png](output_8_3.png)
    


    None
    For the test dataset:
    


    
![png](output_8_5.png)
    


    The test accuracy is 0.827318156579678, the test precision is 0.445141065830721,the test recall is 0.5144927536231884, the test f1 score is 0.47731092436974787, the balanced accuracy is 0.6992135899263483
    


    
![png](output_8_7.png)
    


    None
                               Logit Regression Results                           
    ==============================================================================
    Dep. Variable:              our_label   No. Observations:                 7201
    Model:                          Logit   Df Residuals:                     6881
    Method:                           MLE   Df Model:                          319
    Date:                Tue, 15 Mar 2022   Pseudo R-squ.:                  0.3034
    Time:                        10:19:15   Log-Likelihood:                -2146.5
    converged:                       True   LL-Null:                       -3081.6
    Covariance Type:            nonrobust   LLR p-value:                3.192e-217
    ===================================================================================
                          coef    std err          z      P>|z|      [0.025      0.975]
    -----------------------------------------------------------------------------------
    const              -2.8649      0.339     -8.454      0.000      -3.529      -2.201
    answer_length      -0.0027      0.001     -2.848      0.004      -0.005      -0.001
    q_upper             0.8356      0.457      1.829      0.067      -0.060       1.731
    a_upper            -1.9901      0.584     -3.410      0.001      -3.134      -0.846
    q_model             0.3796      0.118      3.222      0.001       0.149       0.611
    q_do               -0.1721      0.095     -1.803      0.071      -0.359       0.015
    a_model             0.5792      0.263      2.202      0.028       0.064       1.095
    a_imp_dim          -0.8802      0.253     -3.472      0.001      -1.377      -0.383
    q_pos_sentiment     0.7744      0.343      2.259      0.024       0.103       1.446
    a_pos_sentiment     0.7950      0.332      2.392      0.017       0.144       1.446
    q_adverbs           2.0391      0.885      2.304      0.021       0.305       3.774
    q_determiners       1.4054      0.643      2.187      0.029       0.146       2.665
    q_nouns             2.0591      0.404      5.096      0.000       1.267       2.851
    q_numerals          1.8164      0.771      2.355      0.019       0.305       3.328
    a_adpositions       1.9139      0.709      2.698      0.007       0.523       3.304
    a_pronouns          2.1743      0.800      2.719      0.007       0.607       3.742
    q_punc_count        0.0818      0.030      2.685      0.007       0.022       0.141
    a_punc_ratio       -3.6899      1.870     -1.973      0.048      -7.355      -0.024
    jaccard            -2.8094      0.673     -4.171      0.000      -4.129      -1.489
    lev_distance        0.1447      0.061      2.358      0.018       0.024       0.265
    0                  -0.1326      0.374     -0.354      0.723      -0.866       0.601
    1                  -0.4470      0.505     -0.886      0.376      -1.436       0.542
    2                   0.4347      0.370      1.174      0.240      -0.291       1.160
    3                   0.0701      0.387      0.181      0.856      -0.689       0.829
    4                  -0.8410      0.432     -1.948      0.051      -1.687       0.005
    5                  -0.1780      0.295     -0.603      0.546      -0.756       0.400
    6                  -0.4709      0.475     -0.990      0.322      -1.403       0.461
    7                  -0.0756      0.309     -0.244      0.807      -0.681       0.530
    8                   0.6792      0.400      1.700      0.089      -0.104       1.462
    9                   0.2261      0.248      0.912      0.362      -0.260       0.712
    10                 -0.4235      0.238     -1.778      0.075      -0.890       0.043
    11                 -0.1487      0.204     -0.728      0.466      -0.549       0.252
    12                 -0.3254      0.182     -1.787      0.074      -0.682       0.031
    13                  0.4273      0.302      1.416      0.157      -0.164       1.019
    14                  0.4778      0.231      2.071      0.038       0.026       0.930
    15                 -0.1689      0.287     -0.589      0.556      -0.731       0.393
    16                 -0.7814      0.420     -1.861      0.063      -1.604       0.041
    17                  0.3039      0.269      1.131      0.258      -0.223       0.831
    18                 -0.1261      0.417     -0.303      0.762      -0.942       0.690
    19                 -0.1923      0.230     -0.835      0.404      -0.644       0.259
    20                 -0.7981      0.524     -1.523      0.128      -1.825       0.229
    21                  0.3560      0.212      1.677      0.094      -0.060       0.772
    22                  0.0215      0.338      0.064      0.949      -0.641       0.684
    23                 -0.5149      0.258     -1.998      0.046      -1.020      -0.010
    24                  0.2216      0.264      0.839      0.402      -0.296       0.739
    25                  0.3043      0.288      1.058      0.290      -0.260       0.868
    26                 -0.7596      0.464     -1.637      0.102      -1.669       0.150
    27                  0.1593      0.305      0.522      0.602      -0.439       0.757
    28                 -0.4300      0.301     -1.430      0.153      -1.019       0.159
    29                  0.3167      0.185      1.714      0.087      -0.046       0.679
    30                 -0.0624      0.296     -0.211      0.833      -0.642       0.517
    31                  0.3524      0.292      1.206      0.228      -0.220       0.925
    32                  0.0440      0.212      0.208      0.835      -0.371       0.459
    33                  0.0361      0.354      0.102      0.919      -0.657       0.729
    34                  0.0257      0.195      0.131      0.896      -0.357       0.409
    35                 -0.2025      0.241     -0.841      0.400      -0.674       0.269
    36                 -0.2265      0.304     -0.746      0.456      -0.822       0.369
    37                  0.6798      0.345      1.970      0.049       0.003       1.356
    38                 -0.8117      0.370     -2.192      0.028      -1.538      -0.086
    39                 -0.0342      0.194     -0.176      0.860      -0.415       0.346
    40                 -0.2687      0.230     -1.170      0.242      -0.719       0.181
    41                 -0.3400      0.426     -0.799      0.424      -1.174       0.494
    42                  0.1247      0.257      0.485      0.627      -0.379       0.628
    43                  0.0708      0.419      0.169      0.866      -0.750       0.891
    44                  0.4967      0.513      0.968      0.333      -0.509       1.502
    45                 -0.6905      0.233     -2.967      0.003      -1.147      -0.234
    46                  0.4446      0.277      1.605      0.108      -0.098       0.988
    47                 -0.5806      0.356     -1.632      0.103      -1.278       0.117
    48                 -0.4931      0.350     -1.410      0.158      -1.178       0.192
    49                  0.1358      0.286      0.475      0.635      -0.425       0.696
    50                 -0.0901      0.360     -0.250      0.802      -0.796       0.615
    51                 -0.6047      0.575     -1.052      0.293      -1.732       0.522
    52                 -0.3790      0.277     -1.369      0.171      -0.922       0.164
    53                  0.3378      0.398      0.848      0.397      -0.443       1.119
    54                 -0.0926      0.188     -0.492      0.623      -0.462       0.276
    55                  0.2113      0.349      0.606      0.545      -0.472       0.895
    56                  0.5774      0.250      2.306      0.021       0.087       1.068
    57                 -0.3054      0.324     -0.944      0.345      -0.939       0.329
    58                  0.5131      0.297      1.727      0.084      -0.069       1.095
    59                 -1.0391      0.636     -1.635      0.102      -2.285       0.207
    60                 -0.5815      0.432     -1.347      0.178      -1.428       0.265
    61                 -0.6283      0.679     -0.926      0.355      -1.958       0.702
    62                 -0.0431      0.368     -0.117      0.907      -0.764       0.677
    63                 -0.3462      0.211     -1.638      0.101      -0.760       0.068
    64                  0.0218      0.311      0.070      0.944      -0.589       0.632
    65                 -0.1109      0.442     -0.251      0.802      -0.978       0.756
    66                 -0.2917      0.271     -1.079      0.281      -0.822       0.238
    67                 -0.3868      0.156     -2.473      0.013      -0.693      -0.080
    68                  0.2828      0.175      1.613      0.107      -0.061       0.626
    69                  0.3856      0.247      1.559      0.119      -0.099       0.870
    70                 -0.2967      0.269     -1.101      0.271      -0.825       0.231
    71                  0.5519      0.142      3.887      0.000       0.274       0.830
    72                 -0.3833      0.461     -0.832      0.405      -1.286       0.520
    73                 -0.7876      0.306     -2.578      0.010      -1.387      -0.189
    74                 -0.4087      0.381     -1.074      0.283      -1.155       0.337
    75                  0.5671      0.235      2.410      0.016       0.106       1.028
    76                 -0.2888      0.224     -1.292      0.196      -0.727       0.149
    77                  0.2086      0.277      0.753      0.452      -0.335       0.752
    78                 -0.3068      0.361     -0.849      0.396      -1.015       0.402
    79                  0.2109      0.283      0.745      0.456      -0.344       0.766
    80                 -0.6058      0.488     -1.241      0.215      -1.563       0.351
    81                  0.2082      0.335      0.621      0.535      -0.449       0.866
    82                 -0.0997      0.281     -0.355      0.722      -0.650       0.450
    83                 -0.4321      0.377     -1.146      0.252      -1.171       0.307
    84                 -0.4119      0.506     -0.814      0.416      -1.404       0.580
    85                 -0.6936      0.530     -1.308      0.191      -1.733       0.345
    86                 -0.1260      0.098     -1.286      0.198      -0.318       0.066
    87                  0.3582      0.209      1.718      0.086      -0.051       0.767
    88                 -0.2733      0.248     -1.101      0.271      -0.760       0.213
    89                 -0.5621      0.364     -1.544      0.123      -1.276       0.152
    90                 -0.0997      0.119     -0.841      0.400      -0.332       0.133
    91                 -0.3100      0.286     -1.085      0.278      -0.870       0.250
    92                  0.0131      0.250      0.053      0.958      -0.476       0.503
    93                 -0.2601      0.204     -1.272      0.203      -0.661       0.141
    94                 -0.3196      0.354     -0.903      0.367      -1.014       0.374
    95                  0.0389      0.440      0.088      0.929      -0.824       0.902
    96                 -0.2389      0.365     -0.655      0.513      -0.954       0.476
    97                  0.1554      0.211      0.737      0.461      -0.258       0.569
    98                 -0.1235      0.168     -0.737      0.461      -0.452       0.205
    99                  0.2714      0.286      0.951      0.342      -0.288       0.831
    100                -0.1953      0.211     -0.927      0.354      -0.608       0.217
    101                 0.2975      0.202      1.470      0.142      -0.099       0.694
    102                 0.5543      0.309      1.797      0.072      -0.050       1.159
    103                 0.5893      0.280      2.108      0.035       0.041       1.137
    104                 0.2849      0.190      1.500      0.134      -0.087       0.657
    105                -0.1729      0.166     -1.045      0.296      -0.497       0.152
    106                -0.3408      0.393     -0.866      0.386      -1.112       0.430
    107                 0.6434      0.350      1.840      0.066      -0.042       1.329
    108                 0.0832      0.392      0.212      0.832      -0.685       0.852
    109                 0.1376      0.252      0.545      0.586      -0.357       0.632
    110                -0.0670      0.429     -0.156      0.876      -0.908       0.774
    111                 0.2052      0.223      0.920      0.357      -0.232       0.642
    112                -0.2385      0.353     -0.676      0.499      -0.930       0.453
    113                -0.3987      0.410     -0.972      0.331      -1.203       0.406
    114                -0.5145      0.390     -1.319      0.187      -1.279       0.250
    115                -0.3307      0.408     -0.810      0.418      -1.131       0.469
    116                 0.5844      0.261      2.238      0.025       0.073       1.096
    117                 0.0400      0.265      0.151      0.880      -0.480       0.560
    118                -0.0607      0.272     -0.223      0.823      -0.594       0.473
    119                -0.1884      0.222     -0.850      0.395      -0.623       0.246
    120                -0.0997      0.476     -0.209      0.834      -1.033       0.834
    121                 0.5635      0.328      1.720      0.086      -0.079       1.206
    122                -0.4469      0.281     -1.589      0.112      -0.998       0.104
    123                -2.3373      0.907     -2.576      0.010      -4.116      -0.559
    124                -0.0294      0.215     -0.137      0.891      -0.450       0.392
    125                -0.5626      0.192     -2.932      0.003      -0.939      -0.187
    126                -0.4766      0.395     -1.205      0.228      -1.252       0.298
    127                 0.2807      0.285      0.985      0.325      -0.278       0.839
    128                 0.0468      0.158      0.295      0.768      -0.264       0.357
    129                 0.0265      0.397      0.067      0.947      -0.752       0.805
    130                -0.2395      0.416     -0.576      0.565      -1.055       0.576
    131                 0.4196      0.277      1.516      0.130      -0.123       0.962
    132                 0.0083      0.279      0.030      0.976      -0.539       0.556
    133                 0.1599      0.231      0.692      0.489      -0.293       0.613
    134                 0.3483      0.298      1.169      0.242      -0.236       0.932
    135                 0.4409      0.223      1.975      0.048       0.003       0.879
    136                -0.1829      0.325     -0.562      0.574      -0.821       0.455
    137                -0.1862      0.333     -0.559      0.576      -0.839       0.467
    138                 0.0007      0.296      0.002      0.998      -0.580       0.581
    139                 1.4481      0.133     10.903      0.000       1.188       1.708
    140                 0.3043      0.308      0.988      0.323      -0.299       0.908
    141                -0.0766      0.316     -0.242      0.809      -0.697       0.543
    142                 0.1571      0.278      0.565      0.572      -0.388       0.702
    143                 0.3611      0.434      0.833      0.405      -0.489       1.211
    144                 0.1203      0.358      0.336      0.737      -0.581       0.821
    145                 0.0460      0.368      0.125      0.901      -0.676       0.768
    146                 0.5205      0.251      2.072      0.038       0.028       1.013
    147                -0.2704      0.315     -0.859      0.390      -0.887       0.346
    148                 0.0447      0.162      0.276      0.783      -0.273       0.362
    149                 0.4620      0.248      1.863      0.063      -0.024       0.948
    150                 0.1293      0.259      0.499      0.618      -0.379       0.637
    151                -0.5987      0.323     -1.852      0.064      -1.232       0.035
    152                -0.3543      0.345     -1.027      0.304      -1.030       0.322
    153                 0.2455      0.248      0.989      0.323      -0.241       0.732
    154                 0.3392      0.136      2.494      0.013       0.073       0.606
    155                -0.4563      0.371     -1.231      0.218      -1.183       0.270
    156                 0.2803      0.288      0.975      0.330      -0.283       0.844
    157                -1.0513      0.493     -2.134      0.033      -2.017      -0.086
    158                 0.2869      0.308      0.931      0.352      -0.317       0.891
    159                 0.2210      0.168      1.315      0.188      -0.108       0.550
    160                 0.1270      0.211      0.603      0.547      -0.286       0.540
    161                -0.6553      0.200     -3.283      0.001      -1.047      -0.264
    162                -0.2620      0.358     -0.732      0.464      -0.963       0.439
    163                 0.3727      0.286      1.304      0.192      -0.187       0.933
    164                 0.2502      0.238      1.049      0.294      -0.217       0.718
    165                 0.0071      0.269      0.026      0.979      -0.520       0.534
    166                 0.0623      0.307      0.203      0.839      -0.539       0.664
    167                -0.2852      0.256     -1.114      0.265      -0.787       0.217
    168                 0.0049      0.279      0.018      0.986      -0.541       0.551
    169                -0.1785      0.282     -0.633      0.527      -0.732       0.374
    170                -0.3403      0.447     -0.762      0.446      -1.215       0.535
    171                 0.5078      0.296      1.714      0.086      -0.073       1.088
    172                -0.3172      0.156     -2.028      0.043      -0.624      -0.011
    173                -0.3209      0.429     -0.748      0.454      -1.161       0.520
    174                -0.0125      0.184     -0.068      0.946      -0.373       0.348
    175                 0.5194      0.225      2.313      0.021       0.079       0.959
    176                 0.0694      0.290      0.239      0.811      -0.499       0.638
    177                -0.1886      0.354     -0.532      0.595      -0.883       0.506
    178                -0.7298      0.559     -1.307      0.191      -1.824       0.365
    179                 0.0423      0.249      0.170      0.865      -0.445       0.529
    180                 0.5297      0.369      1.434      0.151      -0.194       1.254
    181                -0.1825      0.150     -1.220      0.223      -0.476       0.111
    182                 0.9633      0.245      3.928      0.000       0.483       1.444
    183                 0.2676      0.207      1.294      0.196      -0.138       0.673
    184                -0.4094      0.115     -3.558      0.000      -0.635      -0.184
    185                 0.5384      0.065      8.296      0.000       0.411       0.666
    186                -0.4584      0.140     -3.271      0.001      -0.733      -0.184
    187                -0.1359      0.232     -0.585      0.559      -0.591       0.320
    188                -0.2608      0.356     -0.732      0.464      -0.959       0.437
    189                -0.1006      0.102     -0.986      0.324      -0.300       0.099
    190                -0.3155      0.180     -1.757      0.079      -0.667       0.036
    191                -0.5855      0.319     -1.833      0.067      -1.211       0.040
    192                 0.2636      0.194      1.361      0.173      -0.116       0.643
    193                -0.1652      0.283     -0.584      0.559      -0.719       0.389
    194                 0.2254      0.307      0.733      0.464      -0.377       0.828
    195                -0.0019      0.200     -0.009      0.992      -0.393       0.389
    196                -0.0772      0.320     -0.241      0.810      -0.705       0.551
    197                 0.0832      0.093      0.894      0.371      -0.099       0.266
    198                 0.0952      0.340      0.280      0.779      -0.570       0.761
    199                -0.3773      0.305     -1.236      0.217      -0.976       0.221
    200                -0.0383      0.330     -0.116      0.908      -0.685       0.608
    201                -0.2221      0.400     -0.556      0.579      -1.006       0.562
    202                -0.6854      0.344     -1.991      0.046      -1.360      -0.011
    203                -0.0938      0.285     -0.329      0.742      -0.652       0.464
    204                -0.5213      0.366     -1.426      0.154      -1.238       0.195
    205                -0.0350      0.230     -0.152      0.879      -0.486       0.416
    206                -0.0454      0.358     -0.127      0.899      -0.747       0.656
    207                -0.2913      0.315     -0.925      0.355      -0.909       0.326
    208                -0.5787      0.295     -1.962      0.050      -1.157      -0.001
    209                 0.0767      0.263      0.292      0.770      -0.438       0.591
    210                -0.0850      0.196     -0.435      0.664      -0.468       0.298
    211                 0.4338      0.182      2.379      0.017       0.076       0.791
    212                -0.8722      0.560     -1.558      0.119      -1.970       0.225
    213                -1.0940      0.493     -2.217      0.027      -2.061      -0.127
    214                 0.2174      0.193      1.126      0.260      -0.161       0.596
    215                 0.1894      0.201      0.942      0.346      -0.205       0.584
    216                 0.4069      0.211      1.924      0.054      -0.008       0.821
    217                 0.4577      0.270      1.694      0.090      -0.072       0.987
    218                -0.0323      0.248     -0.130      0.896      -0.519       0.454
    219                 0.3114      0.349      0.892      0.372      -0.373       0.996
    220                -0.0024      0.248     -0.010      0.992      -0.489       0.484
    221                 0.3046      0.287      1.060      0.289      -0.259       0.868
    222                 0.2115      0.193      1.094      0.274      -0.168       0.591
    223                 0.1667      0.253      0.659      0.510      -0.329       0.663
    224                 0.1828      0.336      0.543      0.587      -0.477       0.842
    225                -0.1611      0.150     -1.072      0.284      -0.456       0.134
    226                -0.8439      0.487     -1.732      0.083      -1.799       0.111
    227                 0.6614      0.248      2.663      0.008       0.175       1.148
    228                 0.0027      0.233      0.012      0.991      -0.453       0.459
    229                 0.0528      0.267      0.198      0.843      -0.470       0.576
    230                 0.6549      0.303      2.158      0.031       0.060       1.250
    231                 0.2085      0.207      1.010      0.313      -0.196       0.613
    232                -0.0138      0.384     -0.036      0.971      -0.767       0.739
    233                 0.2590      0.284      0.911      0.363      -0.298       0.816
    234                -0.0768      0.376     -0.204      0.838      -0.814       0.661
    235                -0.6520      0.314     -2.077      0.038      -1.267      -0.037
    236                 0.0895      0.182      0.491      0.624      -0.268       0.447
    237                -0.0361      0.303     -0.119      0.905      -0.629       0.557
    238                 0.1392      0.370      0.376      0.707      -0.586       0.864
    239                -0.1157      0.247     -0.468      0.640      -0.600       0.369
    240                -0.4276      0.307     -1.394      0.163      -1.029       0.174
    241                -0.6027      0.194     -3.109      0.002      -0.983      -0.223
    242                -0.0460      0.408     -0.113      0.910      -0.845       0.753
    243                -0.1262      0.205     -0.615      0.539      -0.528       0.276
    244                 0.3547      0.245      1.447      0.148      -0.126       0.835
    245                 0.6869      0.288      2.382      0.017       0.122       1.252
    246                -0.2581      0.321     -0.804      0.421      -0.887       0.371
    247                 0.1402      0.199      0.706      0.480      -0.249       0.529
    248                 0.2077      0.252      0.825      0.409      -0.286       0.701
    249                 1.1210      0.201      5.586      0.000       0.728       1.514
    250                -0.4326      0.612     -0.707      0.480      -1.633       0.768
    251                -0.4193      0.347     -1.209      0.227      -1.099       0.260
    252                -0.2716      0.390     -0.697      0.486      -1.035       0.492
    253                -0.6911      0.735     -0.940      0.347      -2.133       0.750
    254                 0.0423      0.291      0.145      0.885      -0.529       0.613
    255                 0.2746      0.177      1.551      0.121      -0.072       0.622
    256                 0.8150      0.122      6.664      0.000       0.575       1.055
    257                 0.2232      0.293      0.761      0.446      -0.351       0.798
    258                -0.5558      0.445     -1.250      0.211      -1.427       0.316
    259                -0.2008      0.264     -0.760      0.447      -0.718       0.317
    260                 0.4092      0.271      1.510      0.131      -0.122       0.940
    261                -0.5373      0.425     -1.265      0.206      -1.370       0.295
    262                -0.3816      0.298     -1.279      0.201      -0.966       0.203
    263                 0.0402      0.082      0.489      0.625      -0.121       0.201
    264                 0.7618      0.246      3.091      0.002       0.279       1.245
    265                -0.0490      0.184     -0.266      0.790      -0.409       0.311
    266                -0.3090      0.326     -0.949      0.343      -0.947       0.329
    267                -0.2694      0.404     -0.667      0.505      -1.061       0.522
    268                 0.0509      0.193      0.264      0.791      -0.327       0.428
    269                -0.3093      0.253     -1.225      0.221      -0.804       0.186
    270                 0.1084      0.210      0.516      0.606      -0.303       0.520
    271                -0.0080      0.247     -0.033      0.974      -0.492       0.476
    272                -0.0982      0.218     -0.450      0.653      -0.526       0.330
    273                -0.6098      0.406     -1.503      0.133      -1.405       0.186
    274                -0.0597      0.138     -0.433      0.665      -0.330       0.211
    275                -0.0119      0.315     -0.038      0.970      -0.630       0.606
    276                 0.0668      0.098      0.679      0.497      -0.126       0.260
    277                -0.1563      0.250     -0.624      0.532      -0.647       0.334
    278                -0.1220      0.212     -0.575      0.565      -0.538       0.294
    279                -0.0200      0.155     -0.129      0.897      -0.323       0.283
    280                 0.1692      0.100      1.694      0.090      -0.027       0.365
    281                 0.1386      0.339      0.409      0.683      -0.526       0.803
    282                -0.1169      0.258     -0.453      0.650      -0.622       0.389
    283                -0.6561      0.369     -1.777      0.076      -1.380       0.068
    284                 0.4824      0.242      1.995      0.046       0.008       0.956
    285                -0.3726      0.194     -1.918      0.055      -0.753       0.008
    286                -0.1430      0.132     -1.084      0.278      -0.401       0.115
    287                -0.0527      0.239     -0.220      0.826      -0.522       0.416
    288                -0.7828      0.318     -2.459      0.014      -1.407      -0.159
    289                -0.0846      0.211     -0.401      0.689      -0.498       0.329
    290                 0.4132      0.147      2.814      0.005       0.125       0.701
    291                -0.2192      0.346     -0.634      0.526      -0.896       0.458
    292                 0.2343      0.433      0.542      0.588      -0.613       1.082
    293                -0.3086      0.284     -1.085      0.278      -0.866       0.249
    294                 0.3923      0.348      1.128      0.259      -0.289       1.074
    295                 0.0761      0.104      0.730      0.466      -0.128       0.281
    296                -0.2058      0.108     -1.904      0.057      -0.418       0.006
    297                 0.4239      0.971      0.437      0.662      -1.479       2.327
    298                -3.0558      0.284    -10.754      0.000      -3.613      -2.499
    299                 0.2257      0.199      1.132      0.258      -0.165       0.616
    ===================================================================================
    

We wont go through the whole summary as it is a bit long! We uses the P>|z| to determine statistically significant features to retain.

### Weighted Model


```python
X_train.drop(columns = ['const', 'target_pred', 'pred_binary'], inplace = True)
X_test.drop(columns = ['const', 'target_pred', 'pred_binary'], inplace = True)
```

After using the stats-models Logistic Regression model to select features, we used the Sklearn Logistic Regression model to allow us to weight the data as we have imbalanced classes.

We scaled the data to speed up convergence of the model.


```python
from sklearn.preprocessing import MaxAbsScaler

scaler = MaxAbsScaler()
X_train = pd.DataFrame(scaler.fit_transform(X_train))
X_test = pd.DataFrame(scaler.transform(X_test))
```

For our weighting, we counted the proportion of observations that belonged to each class and turned this into a percentage.



```python
from sklearn.linear_model import LogisticRegression

w1, w2 = 100*df['our_label'].value_counts()/df.shape[0]
w = {0:w2, 1:w1}

results = LogisticRegression(class_weight=w, max_iter = 400).fit(X_train, y_train)

X_train['target_pred'] = results.predict_proba(X_train)[:,1]

X_test['target_pred'] = results.predict_proba(X_test)[:,1]

print(f'For the train dataset:')
print(produce_confusion('Useless', 'Useful', 0.5,  X_train, 'target_pred', y_train))

print(f'For the test dataset:')
print(produce_confusion('Useless', 'Useful', 0.5, X_test, 'target_pred', y_test))
```

    For the train dataset:
    


    
![png](output_15_1.png)
    


    The test accuracy is 0.779613942507985, the test precision is 0.39293598233995586,the test recall is 0.8076225045372051, the test f1 score is 0.5286605286605287, the balanced accuracy is 0.7910878549903602
    


    
![png](output_15_3.png)
    


    None
    For the test dataset:
    


    
![png](output_15_5.png)
    


    The test accuracy is 0.7284841754580789, the test precision is 0.31918505942275044,the test recall is 0.6811594202898551, the test f1 score is 0.4346820809248555, the balanced accuracy is 0.709104300308862
    


    
![png](output_15_7.png)
    


    None
    

## Pipeline implimentation


### Loading the data


```python
import pandas as pd
import numpy as np
```


```python
data = pd.read_csv('appliances_cleaned_final.csv')
data.dropna(inplace=True)
```

### Determine feature types


```python
def feature_catogoriser(df, target):
    num_features = []
    text_features = []
    for feature in df.columns:
        if ((df[feature].dtype) == 'O') & (feature != target):
            text_features.append(feature)
        elif feature != target:
            num_features.append(feature)

    return text_features, num_features
```

The features catogoriser runs over all features in the data frame, labeling Object types as
text columns and others as numerical columns.

We then give a list of features to be used (determined by our model development) and use the 
feature catogoriser to sort them.


```python
cols = ['answer_length', 'q_upper', 'a_upper', 'q_model',
       'q_do', 'a_model', 'a_imp_dim', 'q_pos_sentiment', 'a_pos_sentiment',
       'q_adverbs', 'q_determiners',
       'q_nouns', 'q_numerals', 'a_adpositions', 
       'a_pronouns', 'a_re_joined', 'q_punc_count', 'a_punc_ratio',
       'jaccard', 'lev_distance', 'our_label']


text_features, num_features = feature_catogoriser(data[cols], 'our_label')
```

### Fitting and Evaluating Model

The next series of functions run an experiment and output the model and an evaluation of it.

Experimenter takes our data and feature information along with a dictionary of classifiers 
and some other parameters. It performs our train/test split then runs and evaluates the 
model using each classifier in turn. We experimented with taking an ensemble aproach 
at this stage but stripped it out for readability as it was not used in our final model.


```python
from sklearn.model_selection import train_test_split
from sklearn import metrics


def experimenter(df, target, classifiers,threshold=0.5, text_features=[], num_features=[], 
                 stop_words=None, n_gram=(1,2), chi_k=300):
    features = text_features + num_features
    X = data[features]
    y = data[target]
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=142, stratify=y)
    y_test = y_test.to_frame()
    
    out = pd.DataFrame()
    
    for key, value in classifiers.items():  #for each classifier provided run model and produce scores
        y_test[value], pl = modeling_f(X_train, X_test, y_train, key, features=features, 
                                  chi_k = chi_k, n_gram=n_gram)
        prediction = np.where(y_test[value] > threshold , 1, 0)
        out.loc[value,'accuracy'] = round(metrics.accuracy_score(y_test[target], prediction),3)
        out.loc[value,'precision'] = round(metrics.precision_score(y_test[target], prediction ),3)
        out.loc[value,'recall'] = round(metrics.recall_score(y_test[target], prediction),3)
        out.loc[value,'auc'] = round(metrics.roc_auc_score(y_test[target], y_test[value]),3)
    
    return out, pl
    
```

### The pipeline

We quickly define two functions which sort the columns into numerical and textual for textual features
we combine them into a single string (though in our final model we only use a single textual string).


```python
def select_num_columns(df):
    
    out = df.loc[:, num_features]
    return out
```


```python
def combine_text_columns(df):

    out = df.loc[:, text_features]
    out['text'] =""
    for col in df.columns:
        if col in text_features:
            out.text += " "
            out.text += df[col]
    return out.text
```

We then run the model. In cases where there is only numeric data this is simple - we scale the data and run
the model. If howerver there is textual data the process is more involved:
- We use a feature union to combine two sub pipelines
- The first simply selects and returns our numeric data
- The second selects text data, count vectorises it and then uses SelectKBest to take the most useful


```python
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_selection import chi2, SelectKBest
from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler

def modeling_f(X_train, X_test, y_train, classifier, features, n_gram=(1,2),
               name='experiment', chi_k=300):
    
    # Perform preprocessing
    get_text_data = FunctionTransformer(combine_text_columns, validate=False)
    get_numeric_data = FunctionTransformer(select_num_columns, validate=False)
    
    # Instantiate pipeline: pl
    if text_features == []:
        pl = Pipeline([
                ('scale', MaxAbsScaler()),
                ('clf', classifier)
            ])
    else:
        pl = Pipeline([
                ('union', FeatureUnion(
                    transformer_list = [
                        ('numeric_features', Pipeline([
                            ('selector', get_numeric_data),
                        ])),
                        ('text_features', Pipeline([
                            ('selector', get_text_data),
                            ('vectorizer', CountVectorizer(analyzer = 'word',
                                                           max_features=350,
                                                           ngram_range = n_gram)),
                            ('dim_red', SelectKBest(chi2, k =chi_k))
                        ]))
                     ]
                )),
                ('scale', MaxAbsScaler()),
                ('clf', classifier)
            ])
    
    # Train test split
    X_train = X_train[features]
    X_test = X_test[features]
               
    # Make predictions
    pl.fit(X_train, y_train)
    pred = pl.predict_proba(X_test)[:,1]

    return pred, pl
```

We fit the pipeline on the training data and make our predictions



### Running it

First our Classifier:


```python
from sklearn.linear_model import LogisticRegression

w1, w2 = 100*data['our_label'].value_counts()/data.shape[0]
w = {0:w2, 1:w1}

classifiers = {LogisticRegression(class_weight=w, max_iter=400):'lr'}
```

Now we run our experiment:


```python
out, pl = experimenter(df=data, target='our_label',classifiers=classifiers, 
                       text_features=text_features, num_features=num_features, 
                       chi_k=100, n_gram=(1,3))
print(out)
```

        accuracy  precision  recall    auc
    lr     0.743      0.343   0.739  0.824
    

and finally the result is pickled


```python
from joblib import dump

dump(pl, 'amazon_model.joblib') 
```




    ['amazon_model.joblib']



## Class and Function



### The ideal:

If we were able to impliment "helpfullness rating" as an object, we would be able to initialise within the object our pipeline and the methods which act on both question and answer. This in turn would improve the speed of our function particularly when run on full DataFrames.

We would also be able to initialise different objects with different pipeline inputs to make predictions about different categories of data.

### Function 

Our helpfullness function is a workaround which takes:
- a question string
- an answer string
- a probability threshold which allows us to finetune the models output.
- simple_test: a Boolian which we used for debuging single q/a pairs.

The function creates question and answer objects in order to be able to access thier atributes. It also calculates the jaccard and lev distance. 

We build a DataFrame from a dictionary and load our pickled pipeline which is then used to make a prediction.


```python
def helpfulness_rating(question, answer, threshold=0.5, simple_test=False):
    Q = Helper(question)
    A = Helper(answer)
    lev_matrix = [lev(i, j) for i in Q.tokens for j in A.tokens]
    list_size = [len(Q.tokens), len(A.tokens)]
    final = []
    for i in range(min(list_size)):
        final.append(min(lev_matrix))
        lev_matrix.remove(min(lev_matrix))
    lev_distance = sum(final) / len(final)
    common_words =  Q.unique_words.intersection(A.unique_words)
    jaccard = len(common_words) / (len(Q.unique_words) + len(A.unique_words)) - len(common_words)
    
    # 
    
    model_input = {
        'answer_length': [A.length],
        'q_upper': [Q.upper_ratio],
        'a_upper': [A.upper_ratio] ,
        'q_model': [Q.mention_model],
        'q_do': [Q.asks_do],
        'a_model': [A.mention_model],
        'a_imp_dim': [A.imperial_dimensions],
        'q_pos_sentiment': [Q.positive_sentiment],
        'a_pos_sentiment': [A.positive_sentiment],
        'q_adverbs': [Q.adverb_ratio],
        'q_determiners': [Q.determiner_ratio],
        'q_nouns': [Q.noun_ratio],
        'q_numerals': [Q.numeral_ratio],
        'a_adpositions': [A.adposition_ratio],
        'a_pronouns': [A.pronoun_ratio],
        'a_re_joined': [A.cleaned],
        'q_punc_count': [len(Q.punctuation_marks)],
        'a_punc_ratio': [A.punctuation_mark_ratio],
        'jaccard': [jaccard],
        'lev_distance': [lev_distance],
    }
    X_test = pd.DataFrame.from_dict(model_input)

    text_features, num_features = feature_catogoriser(X_test, "missingno")
    pl = load('amazon_model.joblib')
    pred = pl.predict_proba(X_test)[:,1]
    result = np.where(pred > threshold , "useless", "useful")

    if simple_test:
        print(f"question: {Q.string}")
        print(f"answer: {A.string}")
        print(f"rating: {result}")
    else:
        return pred
```

### A Class Act

The Helper class was intended as a parent class for our question and answer classes. As things stand the question and answer classes have all atributes whereas in our data this is not the case.

Each Helper object contains all of our regular expressions, stopword lists and feature analysis objects. When we initialise an object it is automatically cleaned and has it's features generated. 

We have also used properties to return features that use regex find. There are two additional methods: 
- The cleaning method
- A number replacement method (not currently implimented)



```python
class Helper:
    question_regex = re.compile(r"(^|\s|\W)[\s\w]+\?(\s|\W|$)")
    url_regex = re.compile(r"(http://|https://)?([a-z0-9][a-z0-9\-]*\.)+(com|co\.uk|net|info|edu|ac.uk|org)(\s|$)")
    model_regex = re.compile(r"(^|\s|\W)[Mm]odel(\s|\W|$)")
    how_regex = re.compile(r"(^|\s|\W)[Hh]ow(\s|\W|$)")
    do_regex = re.compile(r"(^|\s|\W)[Dd]o(es)?(\s|\W|$)")
    where_regex = re.compile(r"(^|\s|\W)[Ww]here(\s|\W|$)")
    dimensions_regex = re.compile(r"(^|\s|\W)([Dd]imensions?|[Mm]easurements?|[Ll](e|o)ng(th)?|[Hh]e?ight?)(\s|\W|$)")
    imp_dim_regex = re.compile(r"(^|\s|\W)\d\w?([\"'`‘’“”]|[Ii]nch(es)?|[Ff](oo|ee)?t)(\s|\W|$)")
    met_dim_regex = re.compile(r"(^|\s|\W)\d\w?(([Cc]enti|[Mm]ili)?[Mm]eters?|[CcMm]?[Mm])(\s|\W|$)")
    can_regex = re.compile(r"(^|\s|\W)[Cc]an(\s|\W|$)")
    fix_regex = re.compile(r"(^|\s|\W)[Ff]ix(ing|ed|es)?(\s|\W|$)")
    work_regex = re.compile(r"(^|\s|\W)[Ww]ork(ing|ed|s)?(\s|\W|$)")
    weight_regex = re.compile(r"(^|\s|\W)[Ww]eihg(t|s|ts)?(\s|\W|$)")
    imp_weight_regex = re.compile(r"([Pp]ounds?|lbS?|[Oo]unces?|ozs?)")
    met_weight_regex = re.compile(r"(([Mm]ili)?[Gg]rams?|[Mm]?[Gg])")
    number_regex = re.compile(r"((?=\b)(one\b|two\b|three\b|four\b|five\b|six\b|seven\b|eight\b|nine\b|ten\b|eleven\b|twelve\b|thirteen\b|fourteen\b|fifteen\b|sixteen\b|seventeen\b|eighteen\b|nineteen\b|twenty\b|thirty\b|fou?rty\b|fifty\b|sixty\b|seventy\b|eighty\b|nine?ty\b|hundred\b|thousand\b|half\b|quarters?\b|eighths?\b)\W?(and (a )?)?)+")
    sentiment = SentimentIntensityAnalyzer()
    p_stemmer = PorterStemmer()
    punc_list = [p for p in string.punctuation]
    stopword = [x for x in stopwords.words('english') if x not in ['no', 'should', 'both', 'not']]

    def __init__(self, string):
        self.string = string
        self.cleaned = self.string
        self.tokens = []
        self.number_replacer()
        self.cleaning()

        self.length = len(self.string)
        self.unique_words = set(self.tokens)
        self.polarity_dict = self.sentiment.polarity_scores(self.string)
        self.negative_sentiment = self.polarity_dict['neg']
        self.positive_sentiment = self.polarity_dict['pos']
        self.neutral_sentiment = self.polarity_dict['neu']
        self.compound_sentiment = self.polarity_dict['compound']
        self.tagword_tuples = pos_tag(self.tokens, tagset='universal')
        self.adjectives = [x[0] for x in self.tagword_tuples if x[1] == 'ADJ']
        self.adpositions = [x[0] for x in self.tagword_tuples if x[1] == 'ADP']
        self.adverbs = [x[0] for x in self.tagword_tuples if x[1] == 'ADV']
        self.conjunctions = [x[0] for x in self.tagword_tuples if x[1] == 'CONJ']
        self.determiners = [x[0] for x in self.tagword_tuples if x[1] == 'DET']
        self.nouns = [x[0] for x in self.tagword_tuples if x[1] == 'NOUN']
        self.numerals = [x[0] for x in self.tagword_tuples if x[1] == 'NUM']
        self.particles = [x[0] for x in self.tagword_tuples if x[1] == 'PRT']
        self.pronouns = [x[0] for x in self.tagword_tuples if x[1] == 'PRON']
        self.verbs = [x[0] for x in self.tagword_tuples if x[1] == 'VERB']
        self.punctuation_marks = [x[0] for x in self.tagword_tuples if x[1] == '.']
        self.unknown_words = [x[0] for x in self.tagword_tuples if x[1] == 'X']
        self.adjective_ratio = len([x[0] for x in self.tagword_tuples if x[1] == 'ADJ']) / len(self.tokens)
        self.adposition_ratio = len([x[0] for x in self.tagword_tuples if x[1] == 'ADP']) / len(self.tokens)
        self.adverb_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'ADV']) / len(self.tokens)
        self.conjunction_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'CONJ']) / len(self.tokens)
        self.determiner_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'DET']) / len(self.tokens)
        self.noun_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'NOUN']) / len(self.tokens)
        self.numeral_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'NUM']) / len(self.tokens)
        self.particle_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'PRT']) / len(self.tokens)
        self.pronoun_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'PRON']) / len(self.tokens)
        self.verb_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'VERB']) / len(self.tokens)
        self.punctuation_mark_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == '.']) / len(self.tokens)
        self.unknown_word_ratio  = len([x[0] for x in self.tagword_tuples if x[1] == 'X']) / len(self.tokens)


    @property
    def question_mark(self):
        return 1 if self.question_regex.search(self.string) else 0

    @property
    def has_url(self):
        return 1 if self.url_regex.search(self.string) else 0

    @property
    def mention_model(self):
        return 1 if self.model_regex.search(self.string) else 0

    @property
    def asks_how(self):
        return 1 if self.how_regex.search(self.string) else 0

    @property
    def asks_do(self):
        return 1 if self.do_regex.search(self.string) else 0

    @property
    def asks_where(self):
        return 1 if self.where_regex.search(self.string) else 0

    @property
    def mention_dimensions(self):
        return 1 if self.dimensions_regex.search(self.string) else 0

    @property
    def imperial_dimensions(self):
        return 1 if self.imp_dim_regex.search(self.string) else 0

    @property
    def metric_dimensions(self):
        return 1 if self.met_dim_regex.search(self.string) else 0

    @property
    def asks_can(self):
        return 1 if self.can_regex.search(self.string) else 0

    @property
    def mentions_fix(self):
        return 1 if self.fix_regex.search(self.string) else 0

    @property
    def mentions_work(self):
        return 1 if self.work_regex.search(self.string) else 0

    @property
    def mentions_weight(self):
        return 1 if self.weight_regex.search(self.string) else 0

    @property
    def imperial_weight(self):
        return 1 if self.imp_weight_regex.search(self.string) else 0

    @property
    def metric_weight(self):
        return 1 if self.met_weight_regex.search(self.string) else 0

    @property
    def word_count(self):
        return len(self.tokens)

    @property
    def upper_ratio(self):
        count = 0
        for char in self.string:
            try:
                assert char.isascii() and char.isupper()
                count +=1
            except AssertionError:
                pass
        return count / len(self.string)

    def cleaning(self):
        self.cleaned = self.cleaned.lower()
        self.cleaned = contractions.fix(self.cleaned)
        self.cleaned = re.sub(self.url_regex, "", self.cleaned)
        self.tokens = word_tokenize(self.cleaned)
        self.tokens = [self.p_stemmer.stem(x) for x in self.tokens]
        self.tokens = [y for y in self.tokens if y not in self.punc_list and y not in self.stopword]
        if len(self.tokens) == 0:
            self.tokens = ['EMPTY']
        self.cleaned = " ".join(self.tokens)


    def number_replacer(self):
        searching = True
        transformed = self.string
        while searching:
            convert_words = self.number_regex.search(transformed)
            if convert_words:
                replacement = str(w2n.word_to_num(convert_words.group(0))) + " "
                transformed = self.number_regex.sub(replacement, convert_words.string, 1)
            else:
                searching = False
        return transformed
```
